Enhancing Medication Safety with Retrieval-Augmented Clinical Question Answering
This project explores whether grounding large language models (LLMs) in structured FDA drug label data (via the OpenFDA API) improves factual accuracy and faithfulness in clinical question answering (QA), particularly for high-risk medications.
 
ğŸ“– Project Overview
Medication errors remain a critical issue in clinical care. While LLMs can assist with medication-related QA, they are prone to hallucinations. We implemented and evaluated Retrieval-Augmented Generation (RAG) pipelines using FDA drug label data to enhance answer reliability and accuracy.
ğŸ”— Live Demo: medguardrx.com
 
ğŸ—ï¸ Architecture Summary
 


Query (Clinical Question)
   â†“
Drug Name Extraction (LLM)
   â†“
OpenFDA Drug Label API Lookup
   â†“
Chunking (300 / 600 tokens)
   â†“
Embedding + Vector Store (ChromaDB)
   â†“
Context Retrieval (Cosine Similarity / MMR)
   â†“
LLM Response (gpt-4o / gemini-2.5-pro)
â€¢	Hard grounding prompts: "Answer the question based only on the following context..."
â€¢	Fallback: None (future work: retrieval confidence fallback)
â€¢	Baselines: LLMs with no grounding
 
ğŸ“Š Evaluation
Tool	Metric	Notes
RAGAs	answer_relevancy	
RAGAs (F1)	factual_correctness	Low scores (0.16â€“0.26), context-sensitive
RAGAs	context_recall	Faithfulness improved with grounding
RAGAs	faithfulness	Faithfulness improved with OPENFDA grounding 
LLM-as-judge	quality, correctness, clarity	Less useful than RAGAs
		
 
ğŸ“ Repository Structure
â€¢	/notebooks/ï¼šContains all code related to running and evaluating RAG pipeline
                     - OpenFDA_RAGAs_v1.1.ipynb
â€¢	README.md: Project overview and usage instructions
 
âœ… Key Findings
â€¢	OpenFDA grounding improves faithfulness but does not necessarily always improve accuracy, especially when source coverage is incomplete
â€¢	MMR retrieval occasionally retrieved more diverse or contextually aligned chunks compared to similarity-only retrieval
â€¢	Baseline (ungrounded) responses were often accurate and relevant in our sample set
â€¢	A flexible prompting strategy that encourages context use while not disabling model prior knowledge is likely to lead to more clinically useful answers
 
ğŸš€ Future Directions
â€¢	Expand the dataset with more diverse, clinician-submitted queries
â€¢	Combine OpenFDA + other clinical data sources (e.g., MIMIC_IV)
â€¢	Design RAG prompts with fallback behavior based on retrieval confidence.
 
ğŸ“¦ Requirements and Setup
Please read me
